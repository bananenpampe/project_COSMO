{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook for data curation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook is used to generate a dataset (in the extended .xyz format) of the CSD-2K CSD-1K and CSD-S546 data (CSD-3K+S546_shift_tensors.xyz) and the CSD-500 and CSD-S104 data (CSD-500+104-7_shift_tensors.xyz), containing the atom-wise isotropic chemical shifts and shift tensor extracted from the .magres files given in the original CSD-X data \n",
    "(archived in https://archive.materialscloud.org/record/2019.0023/v1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CSD-500+104-7_shift_tensors.xyz file misses 7 files from the originally reported CSD-500+104.xyz file. I have noticed that there are 7 structures in the CSD-500+104.xyz file (‘JEKPIZ’, ‘NECFIM’, ‘CUMZAM’, ‘ODEJAJ01’, ‘ITOFEE’, ‘WEPBAV’, ‘FOQREK) that have missing .magres files. These structures were excluded from the tensor containing .xyz file. I have done this by returning the intersection of two dicts containing {key(CSD-identifier) : value(e.g. atoms object)} pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A second file helpers.py contains helper functions building CSD-identifier:property dicts, testing functions and a modified .magres parser from the ASE library. This is necessary because either the .magres parser from the ASE (and http://tfgg.me/magres-format/build/html/index.html) are broken, or the .magres files were not written according to specifications. \n",
    "\n",
    "This originates from large isotropic shifts and tensor values not being white-space separated. \n",
    "'m N-6.0351   57.0173   -6.0146   39.7698   98.8410   49.6738   13.0950   46.7860-115.2668'\n",
    "\n",
    "The not so nice fix is to replace each \"-\" with \" -\" in the file and resubstituting the only other two lines containing \"-\" in the .magres file (some unit information and calculator version number)\n",
    "\n",
    "The modified ASE parser takes a modified file object as input instead of a file path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_macroscopic_tensor(lines):\n",
    "    \"\"\"retrieves the macroscopic contribution iso and tensor from a GIPAW FILE as line-list\n",
    "    \"\"\"\n",
    "    for n,line in enumerate(lines):\n",
    "        if  \"Macroscopic shape contribution in ppm:\" in line:\n",
    "            macroscopic_iso = float(line.split()[-1])\n",
    "            macroscopic_tensor = lines[n+1:n+4]\n",
    "            macroscopic_tensor = [ column.rstrip(\"\\n\").split() for column in macroscopic_tensor]\n",
    "            macroscopic_tensor = np.array(macroscopic_tensor,dtype=float)\n",
    "    \n",
    "    return macroscopic_iso, macroscopic_tensor\n",
    "\n",
    "def get_contributions(lines,propstring,istens):\n",
    "    \"\"\"gets contribution iso and tensorial from GIPAW FILE\n",
    "       istens option is for \"core\" contirbution which has no tensor written\n",
    "       and will be added on diagonal\n",
    "    \"\"\"\n",
    "    \n",
    "    props_iso = []\n",
    "    props_tensor = []\n",
    "    \n",
    "    for n, line in enumerate(lines):\n",
    "        \n",
    "        if propstring in line:\n",
    "            iso = float(line.rstrip(\"\\n\").split()[-1])\n",
    "            \n",
    "            if istens is True:\n",
    "                tensor = [ column.rstrip(\"\\n\").split() for column in lines[n+1:n+4]]\n",
    "                tensor = np.array(tensor,dtype=float)\n",
    "            \n",
    "            else:\n",
    "                tensor = np.eye(3)\n",
    "                tensor *= iso\n",
    "        \n",
    "            props_iso.append(iso)\n",
    "            props_tensor.append(tensor)\n",
    "    \n",
    "    return np.array(props_iso).reshape(-1,1), np.array(props_tensor)\n",
    "\n",
    "def get_nmr_contributions(lines):\n",
    "    \"\"\"gets the individual_contributions of GIPAW shift calculations\n",
    "    \"\"\"\n",
    "\n",
    "    tot_iso = {}\n",
    "    tot_tens = {}\n",
    "\n",
    "    iso_makro, tens_makro = get_macroscopic_tensor(lines)\n",
    "    \n",
    "    prop_identifiers = {\"core sigma:\":\"core_sigma\",\"para_oo sigma:\":\"para_oo_sigma\",\\\n",
    "                        \"para_lq sigma:\":\"para_lq_sigma\",\"para sigma:\":\"para_sigma\",\\\n",
    "                        \"dia sigma:\":\"dia_sigma\",\"bare sigma:\":\"bare_sigma\"}\n",
    "\n",
    "    for propstring in [\"core sigma:\", \"para_oo sigma:\", \"para_lq sigma:\", \"para sigma:\", \"dia sigma:\", \"bare sigma:\"]:\n",
    "\n",
    "        if propstring == \"core sigma:\":\n",
    "            iso, tens = get_contributions(lines, propstring, False)\n",
    "\n",
    "        else:\n",
    "            iso, tens = get_contributions(lines, propstring, True)\n",
    "        \n",
    "        identifier = prop_identifiers[propstring]\n",
    "        tot_iso[identifier+\"_iso\"] = iso \n",
    "        tot_tens[identifier+\"_tensor\"] = tens\n",
    "    \n",
    "    #tot_iso[\"shape_contribution_iso\"] = iso_makro\n",
    "    #tot_tens[\"shape_contribution_iso\"] = tens_makro\n",
    "\n",
    "    return tot_iso, tot_tens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ase.io import read,write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#build list of gipaw_file_names of all the datasets\n",
    "datasets_v1 = [\"../ShiftMLv1_datasets/CSD-2k\",\"../ShiftMLv1_datasets/CSD-500\"]\n",
    "datasets_v11 = [\"../ShiftMLv1.1_datasets/CSD-1k\",\"../ShiftMLv1.1_datasets/CSD-S104\",\"../ShiftMLv1.1_datasets/CSD-S546\"]\n",
    "\n",
    "paths = []\n",
    "pathdict = {}\n",
    "\n",
    "\n",
    "for datadir in datasets_v1 + datasets_v11:\n",
    "    paths += glob.glob(datadir + \"/gipaw/*\")\n",
    "\n",
    "for path in paths:\n",
    "    tmp = path.rstrip(\".nmr.out\")\n",
    "    tmp = tmp.split(\"/\")\n",
    "    pathdict[tmp[-1]] = path\n",
    "\n",
    "data_train = read(\"../make_tensor_data/train_tensor/CSD-3k+S546_shift_tensors.xyz\",\":\")\n",
    "data_test = read(\"../make_tensor_data/test_tensor/CSD-500+104-7_shift_tensors.xyz\",\":\")\n",
    "    \n",
    "#[ *glob.glob(data_dir + \"/gipaw/*\") for datadir in [*datasets_v1,*datasets_v11]]\n",
    "\n",
    "#os.path.join(datasets_v1[0],\"gipaw\"))\n",
    "\n",
    "check_consistent_iso = []\n",
    "check_consistent_tensor = []\n",
    "\n",
    "for data in [data_train,data_test]:\n",
    "    for frame in data:\n",
    "        csd_identifier = frame.info[\"NAME\"]\n",
    "        frame_gipaw_path = pathdict[csd_identifier]\n",
    "\n",
    "        with open(frame_gipaw_path,\"r\") as f:\n",
    "            gipaw_file_lines = f.readlines()\n",
    "            tot_iso, tot_tens = get_nmr_contributions(gipaw_file_lines)\n",
    "            shape_iso, shape_tens = get_macroscopic_tensor(gipaw_file_lines)\n",
    "            shape_iso_array = np.array([shape_iso for i in range(len(frame))])\n",
    "            shape_tens_array = np.array([shape_tens for i in range(len(frame))])\n",
    "        \n",
    "        for key, value in tot_iso.items():\n",
    "            frame.arrays.update({key:value})\n",
    "        \n",
    "        for key, value in tot_tens.items():    \n",
    "            frame.arrays.update({key:value.reshape(-1,9)})\n",
    "        \n",
    "        frame.arrays.update({\"shape_contribution_iso\":shape_iso_array})\n",
    "        frame.arrays.update({\"shape_contribution_tensor\":shape_tens_array.reshape(-1,9)})\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        iso_shifts = frame.arrays['cs_iso'].reshape(-1,1)\n",
    "        tensor_shifts = frame.arrays['cs_tensor']\n",
    "\n",
    "\n",
    "        int_iso = [frame.arrays[i].reshape(-1,1) for i in ['core_sigma_iso', 'para_oo_sigma_iso', 'para_lq_sigma_iso', 'para_sigma_iso', 'dia_sigma_iso', 'bare_sigma_iso', 'shape_contribution_iso']]\n",
    "        int_tens = [frame.arrays[i] for i in ['core_sigma_tensor', 'para_oo_sigma_tensor', 'para_lq_sigma_tensor', 'para_sigma_tensor', 'dia_sigma_tensor', 'bare_sigma_tensor', 'shape_contribution_tensor']]\n",
    "        \n",
    "        check_consistent_iso.append(np.allclose(iso_shifts,np.sum(int_iso,axis=0),atol=0.035))\n",
    "        check_consistent_tensor.append(np.allclose(tensor_shifts,np.sum(int_tens,axis=0),atol=0.035))\n",
    "        \n",
    "        \n",
    "\n",
    "#\n",
    "\"\"\"\n",
    "if (False in check_consistent_tensor) is False:\n",
    "    if (False in check_consistent_iso) is False:\n",
    "        write(\"CSD-3k+S546_shift_tensors_components.xyz\",data_train,format=\"extxyz\")        \n",
    "        write(\"CSD-500+104-7_shift_tensors_components.xyz\",data_test,format=\"extxyz\")\n",
    "\"\"\"\n",
    "#loop through CSD-3K/CSD-500+100 Dataset and get CSD identifier from .info dict\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = read(\"CSD-3k+S546_shift_tensors_components.xyz\",\":\")\n",
    "data_test = read(\"CSD-500+104-7_shift_tensors_components.xyz\",\":\")        \n",
    "\n",
    "check_consistent_iso = []\n",
    "check_consistent_tensor = []\n",
    "\n",
    "\n",
    "for data in [data_train,data_test]:\n",
    "    for frame in data:        \n",
    "        \n",
    "        \n",
    "        iso_shifts = frame.arrays['cs_iso'].reshape(-1,1)\n",
    "        tensor_shifts = frame.arrays['cs_tensor']\n",
    "\n",
    "\n",
    "        int_iso = [frame.arrays[i].reshape(-1,1) for i in ['core_sigma_iso', 'para_oo_sigma_iso', 'para_lq_sigma_iso', 'para_sigma_iso', 'dia_sigma_iso', 'bare_sigma_iso', 'shape_contribution_iso']]\n",
    "        int_tens = [frame.arrays[i] for i in ['core_sigma_tensor', 'para_oo_sigma_tensor', 'para_lq_sigma_tensor', 'para_sigma_tensor', 'dia_sigma_tensor', 'bare_sigma_tensor', 'shape_contribution_tensor']]\n",
    "        \n",
    "        check_consistent_iso.append(np.allclose(iso_shifts,np.sum(int_iso,axis=0),atol=0.035))\n",
    "        check_consistent_tensor.append(np.allclose(tensor_shifts,np.sum(int_tens,axis=0),atol=0.035))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "False in check_consistent_iso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "False in [True,True,True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "False in check_consistent_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame = data_train[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[35.89],\n",
       "       [35.89],\n",
       "       [35.89],\n",
       "       [35.89],\n",
       "       [ 2.43],\n",
       "       [ 2.43],\n",
       "       [ 2.43],\n",
       "       [ 2.43],\n",
       "       [ 3.99],\n",
       "       [ 3.99],\n",
       "       [ 3.99],\n",
       "       [ 3.99],\n",
       "       [43.06],\n",
       "       [43.06],\n",
       "       [43.06],\n",
       "       [43.06],\n",
       "       [22.7 ],\n",
       "       [22.7 ],\n",
       "       [22.7 ],\n",
       "       [22.7 ],\n",
       "       [58.12],\n",
       "       [58.12],\n",
       "       [58.12],\n",
       "       [58.12],\n",
       "       [27.33],\n",
       "       [27.33],\n",
       "       [27.33],\n",
       "       [27.33],\n",
       "       [18.39],\n",
       "       [18.39],\n",
       "       [18.39],\n",
       "       [18.39],\n",
       "       [24.28],\n",
       "       [24.28],\n",
       "       [24.28],\n",
       "       [24.28],\n",
       "       [24.71],\n",
       "       [24.71],\n",
       "       [24.71],\n",
       "       [24.71],\n",
       "       [25.05],\n",
       "       [25.05],\n",
       "       [25.05],\n",
       "       [25.05],\n",
       "       [25.07],\n",
       "       [25.07],\n",
       "       [25.07],\n",
       "       [25.07]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iso_shifts.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48, 9)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_tens[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48, 9)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(int_tens,axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48, 1)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(int_iso,axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([35.89, 35.89, 35.89, 35.89,  2.43,  2.43,  2.43,  2.43,  3.99,\n",
       "        3.99,  3.99,  3.99, 43.06, 43.06, 43.06, 43.06, 22.7 , 22.7 ,\n",
       "       22.7 , 22.7 , 58.12, 58.12, 58.12, 58.12, 27.33, 27.33, 27.33,\n",
       "       27.33, 18.39, 18.39, 18.39, 18.39, 24.28, 24.28, 24.28, 24.28,\n",
       "       24.71, 24.71, 24.71, 24.71, 25.05, 25.05, 25.05, 25.05, 25.07,\n",
       "       25.07, 25.07, 25.07])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iso_shifts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[35.89],\n",
       "       [35.89],\n",
       "       [35.89],\n",
       "       [35.89],\n",
       "       [ 2.42],\n",
       "       [ 2.42],\n",
       "       [ 2.42],\n",
       "       [ 2.42],\n",
       "       [ 3.99],\n",
       "       [ 3.99],\n",
       "       [ 3.99],\n",
       "       [ 3.99],\n",
       "       [43.07],\n",
       "       [43.07],\n",
       "       [43.07],\n",
       "       [43.07],\n",
       "       [22.69],\n",
       "       [22.69],\n",
       "       [22.69],\n",
       "       [22.69],\n",
       "       [58.1 ],\n",
       "       [58.1 ],\n",
       "       [58.1 ],\n",
       "       [58.1 ],\n",
       "       [27.34],\n",
       "       [27.34],\n",
       "       [27.34],\n",
       "       [27.34],\n",
       "       [18.39],\n",
       "       [18.39],\n",
       "       [18.39],\n",
       "       [18.39],\n",
       "       [24.28],\n",
       "       [24.28],\n",
       "       [24.28],\n",
       "       [24.28],\n",
       "       [24.7 ],\n",
       "       [24.7 ],\n",
       "       [24.7 ],\n",
       "       [24.7 ],\n",
       "       [25.05],\n",
       "       [25.05],\n",
       "       [25.05],\n",
       "       [25.05],\n",
       "       [25.07],\n",
       "       [25.07],\n",
       "       [25.07],\n",
       "       [25.07]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48, 7)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.hstack(int_iso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['numbers', 'positions', 'cs_tensor', 'cs_iso', 'core_sigma_iso', 'para_oo_sigma_iso', 'para_lq_sigma_iso', 'para_sigma_iso', 'dia_sigma_iso', 'bare_sigma_iso', 'core_sigma_tensor', 'para_oo_sigma_tensor', 'para_lq_sigma_tensor', 'para_sigma_tensor', 'dia_sigma_tensor', 'bare_sigma_tensor', 'shape_contribution_iso', 'shape_contribution_tensor'])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train[0].arrays.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ma = np.eye(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "this = np.array([ma for i in range(30)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 3, 3)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "this.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 30)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.tile(ma,10).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "adict = {\"a\":\"c\",\"b\":\"d\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "c\n",
      "b\n",
      "d\n"
     ]
    }
   ],
   "source": [
    "for key,value in adict.items():\n",
    "    print(key)\n",
    "    print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Atom('S', [4.37754024, 2.95767432, 3.0668442], index=0)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train[0]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4150.0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2e03 + 1e03 + 104 + 546 + 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4150"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from ase.io import read, write\n",
    "import numpy as np\n",
    "\n",
    "from helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/ssd/scratch/kellner/COSMO_project/make_tensor_data/CSD-500/magres/*magres\n",
      "/ssd/scratch/kellner/COSMO_project/make_tensor_data/CSD-S104/magres/*magres\n"
     ]
    }
   ],
   "source": [
    "#Build dictionary with {key: STRUCTURE-CSD-NAME value: atoms object}\n",
    "\n",
    "extyz_dict = build_extxy_dict(\"CSD-500+S104.xyz\")\n",
    "\n",
    "\n",
    "#da\n",
    "datasets = [\"CSD-500\",\"CSD-S104\"] #test directories\n",
    "contained_in = \"magres\"\n",
    "extension = \"*magres\"\n",
    "\n",
    "#build path\n",
    "#./CSD-500/magres/*magres\n",
    "\n",
    "extyz_dict_tens = {}\n",
    "\n",
    "for dataset in datasets:\n",
    "    DATASETPATH = os.path.join(os.getcwd(),dataset,contained_in,extension)\n",
    "    files = glob.glob(DATASETPATH)\n",
    "    print(DATASETPATH)\n",
    "    for n, file in enumerate(files):\n",
    "        structname = file.rstrip(\".nmr.magres\").split(\"/\")[-1]\n",
    "        extyz_dict_tens.update({structname : None})\n",
    "\n",
    "#build intersection of two sets (.xyz files) necessary for train set due to the 7 missing files\n",
    "final_dict = {x:extyz_dict[x] for x in extyz_dict \n",
    "                              if x in extyz_dict_tens}  \n",
    "\n",
    "# else, just do:\n",
    "# final_dict = {x:extyz_dict[x] for x in extyz_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build dictionary with {key: STRUCTURE-CSD-NAME value: STATUS}\n",
    "\n",
    "all_ids = generate_status_dict(\"CSD-3k+S546.xyz\",\"PASSING\")\n",
    "souspicious = generate_status_dict(\"./frames_status/frames_suspicious.xyz\",\"SUSPICIOUS\")\n",
    "outliers = generate_status_dict(\"./frames_status/frames_blatant_outliers.xyz\",\"FAIL\")\n",
    "\n",
    "all_ids.update(souspicious) \n",
    "all_ids.update(outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/matthiaskellner/Desktop/EPFL_2021/COSMO_project/make_tensor_data/CSD-500/magres/*magres\n",
      "/Users/matthiaskellner/Desktop/EPFL_2021/COSMO_project/make_tensor_data/CSD-S104/magres/*magres\n"
     ]
    }
   ],
   "source": [
    "#loop through datasets (magres directories and files that are contained in the latter)\n",
    "#extract the CSD name from the file name\n",
    "# read and generate atoms objects from .magres files\n",
    "# remove atoms.info pairs that are garbage\n",
    "# if set_status (for training) is True: write status to info dict\n",
    "# flatten shift tensor and add it with another name\n",
    "# change coordinates to coordinates from .extyz file (higher precision) \n",
    "# \n",
    "\n",
    "\n",
    "# Directories, where .magresfiles are located\n",
    "datasets= [\"CSD-500\",\"CSD-S104\"] #[\"CSD-2k\",\"CSD-1k\",\"CSD-S546\"]\n",
    "contained_in = \"magres\"\n",
    "extension = \"*magres\"\n",
    "\n",
    "structs = []\n",
    "\n",
    "set_status=False\n",
    "\n",
    "for dataset in datasets:\n",
    "    #build combined filepaths from the working directory and \n",
    "    DATASETPATH = os.path.join(os.getcwd(),dataset,contained_in,extension)\n",
    "    files = glob.glob(DATASETPATH)\n",
    "    print(DATASETPATH)\n",
    "    \n",
    "    for n, file in enumerate(files):\n",
    "        structname = file.rstrip(\".nmr.magres\").split(\"/\")[-1]\n",
    "        #print(structname)\n",
    "        \n",
    "        #try:\n",
    "        with open(file) as f:\n",
    "            fd = f.read()\n",
    "            fd = fd.replace(\"-\",\" -\")\n",
    "            fd = fd.replace(\"units sus 10^ -6.cm^3.mol^ -1\",\"units sus 10^-6.cm^3.mol^-1\")\n",
    "            fd = fd.replace(\"#$magres -abinitio -v1.0\",\"#$magres-abinitio-v1.0\")\n",
    "            fd = fd.replace(\"QE -GIPAW 5.x\",\"QE-GIPAW 5.x\")\n",
    "            atoms = read_magres_modified(fd)\n",
    "            #print(fd)\n",
    "       \n",
    "        \n",
    "        if set_status is True:\n",
    "            atoms.info.update({\"STATUS\":all_ids[structname]})\n",
    "        \n",
    "        #-----flatten TENSOR-----\n",
    "        atoms.arrays.update({\"cs_tensor\": atoms.arrays[\"ms\"].reshape((-1,9))})\n",
    "        atoms.info.update({\"magres_units\": {'cs_tensor': 'ppm', 'cs_iso': 'ppm'}})\n",
    "            \n",
    "        \n",
    "        #----remove labels and incices\n",
    "        atoms.arrays.pop(\"ms\")\n",
    "        atoms.arrays.pop(\"indices\")\n",
    "        atoms.arrays.pop(\"labels\")\n",
    "        \n",
    "        #remove garbage from comments\n",
    "        atoms.info.pop(\"magresblock_calculation\")\n",
    "        \n",
    "        #check if structname is in final dict:\n",
    "        #nescessary for -7 files. probably to complicated\n",
    "        if structname in final_dict:\n",
    "            atoms.info.update({\"NAME\":structname})\n",
    "            atoms.info.update({\"ENERGY\": final_dict[structname].info[\"ENERGY\"]})\n",
    "            atoms.set_positions(final_dict[structname].get_positions())\n",
    "            atoms.set_cell(final_dict[structname].get_cell())\n",
    "            atoms.arrays.update({\"cs_iso\": final_dict[structname].arrays[\"CS\"]})\n",
    "            structs.append(atoms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write(\"CSD-500+104-7_shift_tensors.xyz\",structs,format=\"extxyz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_plausibility(\"./test_tensor/CSD-500+104-7_shift_tensors.xyz\",\"CSD-500+S104.xyz\") \n",
    "#check if PBC, cell, coordinate and shifts are transferred correctly\n",
    "# diagonalizes shift tensor and takes average of eigenvalues. compares the average to the iso shift\n",
    "# to ensure that tensor values are written correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#can be used to check if status was written correctly\n",
    "test_status(\"./train_tensor/CSD-3k+S546_shift_tensors.xyz\",all_ids) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[True, False, False, True]\n",
      "False\n",
      "[True, False, True, True]\n",
      "False\n",
      "[False, True, True, True]\n",
      "False\n",
      "[True, True, True, False]\n",
      "False\n",
      "[True, False, True, True]\n",
      "False\n",
      "[True, False, True, True]\n",
      "False\n",
      "[True, True, True, False]\n",
      "False\n",
      "[False, True, True, True]\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "struct_iso_good = read(\"teststructs_iso_good.xyz\",format=\"extxyz\")\n",
    "struct_tensor_good = read(\"teststructs_tens_good.xyz\",format=\"extxyz\")\n",
    "\n",
    "#-----testing the comparison helper functions\n",
    "bad_structs_iso = [read(this,format=\"extxyz\") for this in [\"teststructs_iso_bad_no_PBC.xyz\",\"teststructs_iso_bad_cell.xyz\",\"teststructs_iso_bad_coordinates.xyz\",\"teststructs_iso_bad_shift.xyz\"]]\n",
    "bad_structs_tens = [read(this,format=\"extxyz\") for this in [\"teststructs_tens_bad_no_PBC.xyz\",\"teststructs_tens_bad_cell.xyz\", \"teststructs_tens_bad_shift.xyz\",\"teststructs_tens_bad_coordinates.xyz\"]]\n",
    "\n",
    "for bad_struct in bad_structs_iso:\n",
    "    print(compaire(bad_struct,struct_tensor_good))\n",
    "for bad_struct in bad_structs_tens:\n",
    "    print(compaire(struct_iso_good,bad_struct))\n",
    "    \n",
    "compaire(struct_iso_bad,struct_tensor_bad)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
